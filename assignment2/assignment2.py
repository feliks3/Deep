# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XHr6f46TY6Fh1tcugavXRkw2qqWhHYo8
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization, InputLayer
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV
from tensorflow.keras.utils import to_categorical
from keras.applications import VGG16
from keras.applications import VGG19
from keras.layers import GlobalMaxPooling2D
from keras import Model
from keras.applications.resnet50 import ResNet50
import time

# %matplotlib inline

"""### unpack file"""

# tools

def unpickle(file):
    import pickle
    with open(file, 'rb') as fo:
        dict = pickle.load(fo, encoding='bytes')
    return dict

path = "/content/drive/My Drive/Colab Notebooks/code/"
file1 = unpickle(path + "data_batch_1")
file2 = unpickle(path + "data_batch_2")
file3 = unpickle(path + "data_batch_3")
file4 = unpickle(path + "data_batch_4")
file5 = unpickle(path + "data_batch_5")
file6 = unpickle(path + "test_batch")

def plot_history(history):
    # Plot the results (shifting validation curves appropriately)
    plt.figure(figsize=(8,5))
    n = len(history.history['accuracy'])
    plt.plot(np.arange(0,n),history.history['accuracy'], color='orange')
    plt.plot(np.arange(0,n),history.history['loss'],'b')
    plt.plot(np.arange(0,n)+0.5,history.history['val_accuracy'],'r')  # offset both validation curves
    plt.plot(np.arange(0,n)+0.5,history.history['val_loss'],'g')
    plt.legend(['Train Acc','Train Loss','Val Acc','Val Loss'])
    plt.grid(True)
    plt.gca().set_ylim(0, 2) # set the vertical range to [0-2] 
    plt.show() 

def plot_acc_history(history):
    # Plot the results (shifting validation curves appropriately)
    plt.figure(figsize=(8,5))
    n = len(history.history['acc'])
    plt.plot(np.arange(0,n),history.history['acc'], color='orange')
    plt.plot(np.arange(0,n),history.history['loss'],'b')
    plt.plot(np.arange(0,n)+0.5,history.history['val_acc'],'r')  # offset both validation curves
    plt.plot(np.arange(0,n)+0.5,history.history['val_loss'],'g')
    plt.legend(['Train Acc','Train Loss','Val Acc','Val Loss'])
    plt.grid(True)
    plt.gca().set_ylim(0, 2) # set the vertical range to [0-2] 
    plt.show()

"""### data processing
* normalization
* one hot
* separate training set into 2 parts, training set and validation set
"""

# data processing
# normalization
# one hot
# separate training set into 2 parts, training set and validation set

train_data_ori = np.r_[file1[b'data'],file2[b'data'],file3[b'data'],file4[b'data'],file5[b'data']]
train_label_ori = np.r_[file1[b'labels'],file2[b'labels'],file3[b'labels'],file4[b'labels'],file5[b'labels']]

test_data_ori = file6[b'data']
test_label_ori = np.r_[file6[b'labels']]

plt.hist(train_label_ori,bins=10)
plt.title("training data distribution")
plt.show()
plt.hist(test_label_ori,bins=10)
plt.title("testing data distribution")
plt.show()

train_data_ori = np.transpose(train_data_ori.reshape(-1, 3, 32, 32), (0, 2, 3, 1))
test_data_ori = np.transpose(test_data_ori.reshape(-1, 3, 32, 32), (0, 2, 3, 1))

train_data_ori = train_data_ori/255
test_data_ori = test_data_ori/255


# one hot
train_label_one_hot_ori = to_categorical(train_label_ori, 10)
test_label_one_hot = to_categorical(test_label_ori, 10)

print("one hot sample" , train_label_one_hot_ori[0])

# split the training set into training set and validation set
train_data, val_data, train_label, val_label = train_test_split(train_data_ori, train_label_one_hot_ori, train_size=0.8, random_state=42, stratify=train_label_one_hot_ori)
test_data, test_label = test_data_ori, test_label_one_hot

print("training data shape ", train_data.shape, train_label.shape)
print("validation data shape ", val_data.shape, val_label.shape)
print("testing data shape ", test_data.shape, test_label.shape)

"""## LeNet 5

### original form
"""

def LeNet_model():
    model = keras.Sequential()
    model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(32, 32, 3)))
    model.add(MaxPool2D(strides=2))
    model.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu'))
    model.add(MaxPool2D(strides=2))
    model.add(Flatten())
    model.add(Dense(256, activation='relu'))
    model.add(Dense(84, activation='relu'))
    model.add(Dense(10, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

leNet_model = LeNet_model()

early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True) 
lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
n_epochs = 100
batch_size = 128

tic = time.time()
LeNet_history = leNet_model.fit(train_data, train_label, epochs=n_epochs, validation_data=(val_data, val_label),batch_size=batch_size, callbacks=[early_stopping_cb, lr_scheduler])
tok = time.time()
print((tok - tic)*1000 , "ms")

leNet_evaluation = leNet_model.evaluate(test_data, test_label)
leNet_test_acc = leNet_evaluation[1]
leNet_train_acc = np.max(LeNet_history.history['accuracy'])
leNet_val_acc = np.max(LeNet_history.history['val_accuracy'])
print("accuracy of the network on the training datasets is " , f'{leNet_train_acc * 100:.2f}%')
print("accuracy of the network on the validation datasets is " , f'{leNet_val_acc * 100:.2f}%')
print("accuracy of the network on the test datasets is ", f'{leNet_test_acc * 100:.2f}%')

plot_history(LeNet_history)

plt.plot(LeNet_history.epoch, LeNet_history.history["lr"], "bo-")
plt.xlabel("Epoch")
plt.ylabel("Learning Rate", color='b')
plt.tick_params('y', colors='b')
#plt.gca().set_xlim(0, n_epochs - 1)
plt.grid(True)

ax2 = plt.gca().twinx()
ax2.plot(LeNet_history.epoch, LeNet_history.history["val_loss"], "r^-")
ax2.set_ylabel('Validation Loss', color='r')
ax2.tick_params('y', colors='r')

plt.title("Reduce LR on Plateau", fontsize=14)
plt.show()

print("LeNet-5 architecture:")
leNet_model.summary()

"""### tuning key parameters with cross validation
* actfn=["elu", "relu"]
* optimizer=[keras.optimizers.Adam,keras.optimizers.SGD] 
* learningrate=[0.1,0.01,0.001,0.0001]
"""

# Some key parameters
learningrate = 1   
batch_size = 128
n_epochs = 100

def model_leNet_factory(actfn="relu", optimizer=keras.optimizers.Adam, learningrate=0.001):
    # LeNet-5
    model = keras.Sequential()
    model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation=actfn, input_shape=(32, 32, 3)))
    model.add(MaxPool2D(strides=2))
    model.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation=actfn))
    model.add(MaxPool2D(strides=2))
    model.add(Flatten())
    model.add(Dense(256, activation=actfn))
    model.add(Dense(84, activation=actfn))
    model.add(Dense(10, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer=optimizer(lr=learningrate), metrics=['accuracy'])
    return model

def randomSearch_LeNet(model):
    keras_cls = keras.wrappers.scikit_learn.KerasClassifier(model)

    param_distribs = {
        "actfn": ["elu","relu",],
        "optimizer": [keras.optimizers.Adam, keras.optimizers.SGD],
        "learningrate": [0.0001*learningrate, 0.001*learningrate, 0.01*learningrate, 0.1*learningrate],
    }

    timeBeforeTraining = time.time()
    early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True) 
    lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
    rnd_search_cv_dnn = RandomizedSearchCV(keras_cls, param_distribs, n_iter=10, cv=3)
    rnd_search_cv_dnn.fit(train_data, train_label, epochs=n_epochs, validation_data=(val_data, val_label),batch_size=batch_size, callbacks=[early_stopping_cb, lr_scheduler])
    timeAfterTraining = time.time()
    print("the training last ", np.round(timeAfterTraining - timeBeforeTraining, 3), " seconds")

    return rnd_search_cv_dnn

model_LeNet_with_para = randomSearch_LeNet(model_leNet_factory)

print("best parameter is ", model_LeNet_with_para.best_params_)
print("best score is ", model_LeNet_with_para.best_score_)
model_LeNet_with_para_history = model_LeNet_with_para.best_estimator_.model.history
plot_history(model_LeNet_with_para_history)

"""### best parameter is  {'optimizer': 'Adam', 'learningrate': 0.001, 'actfn': 'relu'}
### best score is  0.6737750172615051
### valadation accuracy is 0.56, training accuracy is 1, it is overfit, we need to drop some parts to deal with overfit, and a drop out layer after the first max pooling layer
"""

def model_leNet_drop_factory():
    # LeNet-5
    learningrate=0.001
    actfn = "relu"
    optimizer = keras.optimizers.Adam

    model = keras.Sequential()
    model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(32, 32, 3)))
    model.add(MaxPool2D(strides=2))
    model.add(keras.layers.Dropout(0.5))
    model.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu'))
    model.add(MaxPool2D(strides=2))
    model.add(Flatten())
    model.add(Dense(256, activation='relu'))
    model.add(Dense(84, activation='relu'))
    model.add(Dense(10, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer=optimizer(lr=learningrate), metrics=['accuracy'])
    return model

leNet_model_drop = model_leNet_drop_factory()

early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True) 
lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
n_epochs = 100
batch_size = 128
tic = time.time()
LeNet_drop_history = leNet_model_drop.fit(train_data, train_label, epochs=n_epochs, validation_data=(val_data, val_label),batch_size=batch_size, callbacks=[early_stopping_cb, lr_scheduler])
tok = time.time()
print((tok - tic)*1000 , "ms")

leNet_drop_evaluation = leNet_model_drop.evaluate(test_data, test_label)
leNet_drop_train_acc = np.max(LeNet_drop_history.history['accuracy'])
leNet_drop_val_acc = np.max(LeNet_drop_history.history['val_accuracy'])
leNet_drop_test_acc = leNet_drop_evaluation[1]
print("training datasets accuracy is " , f'{leNet_drop_train_acc * 100:.2f}%')
print("validation datasets accuracy is " , f'{leNet_drop_val_acc * 100:.2f}%')
plot_history(history)

def model_leNet_drop_factory():
    # LeNet-5
    learningrate=0.001
    actfn = "relu"
    optimizer = keras.optimizers.Adam

    model = keras.Sequential()
    model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(32, 32, 3)))
    model.add(MaxPool2D(strides=2))
    model.add(keras.layers.Dropout(0.5))
    model.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu'))
    model.add(MaxPool2D(strides=2))
    model.add(keras.layers.Dropout(0.5))
    model.add(Flatten())
    model.add(Dense(256, activation='relu'))
    model.add(Dense(84, activation='relu'))
    model.add(Dense(10, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer=optimizer(lr=learningrate), metrics=['accuracy'])
    return model

leNet_model_drop = model_leNet_drop_factory()

early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True) 
lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
n_epochs = 100
batch_size = 128
tic = time.time()
LeNet_drop_history = leNet_model_drop.fit(train_data, train_label, epochs=n_epochs, validation_data=(val_data, val_label),batch_size=batch_size, callbacks=[early_stopping_cb, lr_scheduler])
tok = time.time()
print((tok - tic)*1000 , "ms")

leNet_drop_train_acc = np.max(LeNet_drop_history.history['accuracy'])
leNet_drop_val_acc = np.max(LeNet_drop_history.history['val_accuracy'])
print("training datasets accuracy is " , f'{leNet_drop_train_acc * 100:.2f}%')
print("validation datasets accuracy is " , f'{leNet_drop_val_acc * 100:.2f}%')
plot_history(history)

"""#### but the score is too low, we have some options to make the architecture more flexible to increase the accuracy.
1. increase convolutional layer filters
2. add dense layer units
3. add more layers
4. BatchNormalization before every Convolutional Layer.
"""

# 1, increase convolutional layer filters
# 2, add dense layer units

def model_leNet_size_factory(filter_size=1, dense_units_size=1):
    # LeNet-5
    learningrate=0.001
    actfn = "relu"
    optimizer = keras.optimizers.Adam

    model = keras.Sequential()
    model.add(layers.Conv2D(filters=32*filter_size, kernel_size=(5, 5), activation=actfn, input_shape=(32,32,3)))
    model.add(MaxPool2D(strides=2))
    model.add(keras.layers.Dropout(0.5))
    model.add(layers.Conv2D(filters=48*filter_size, kernel_size=(5, 5), activation=actfn))
    model.add(MaxPool2D(strides=2))
    model.add(keras.layers.Dropout(0.5))
    model.add(layers.Flatten())
    model.add(layers.Dense(units=256*dense_units_size, activation=actfn))
    model.add(layers.Dense(units=84*dense_units_size, activation=actfn))
    model.add(layers.Dense(units=10, activation = 'softmax'))
    model.compile(loss="categorical_crossentropy", optimizer=optimizer(lr=learningrate), metrics=["accuracy"])   
    return model

def randomSearch_LeNet_size(model):
    keras_cls = keras.wrappers.scikit_learn.KerasClassifier(model)

    param_distribs = {
        "filter_size": range(1,4),
        "dense_units_size": range(1,4)
        }

    timeBeforeTraining = time.time()
    early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True) 
    lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
    n_epochs = 100
    
    rnd_search_cv_dnn = RandomizedSearchCV(keras_cls, param_distribs, n_iter=10, cv=3)
    rnd_search_cv_dnn.fit(train_data, train_label, epochs=n_epochs, validation_data=(val_data, val_label), callbacks=[early_stopping_cb, lr_scheduler])
    timeAfterTraining = time.time()
    print("the training last ", np.round(timeAfterTraining - timeBeforeTraining, 3), " seconds")

    return rnd_search_cv_dnn


model_LeNet_with_size = randomSearch_LeNet_size(model_leNet_size_factory)

print("best parameter is ", model_LeNet_with_size.best_params_)
print("best score is ", model_LeNet_with_size.best_score_)
model_LeNet_with_size_history = model_LeNet_with_size.best_estimator_.model.history
plot_history(model_LeNet_with_size_history)

def model_leNet_size_factory(filter_size=3, dense_units_size=2):
    # LeNet-5
    learningrate=0.001
    actfn = "relu"
    optimizer = keras.optimizers.Adam

    model = keras.Sequential()
    model.add(layers.Conv2D(filters=32*filter_size, kernel_size=(5, 5), activation=actfn, input_shape=(32,32,3)))
    model.add(MaxPool2D(strides=2))
    model.add(keras.layers.Dropout(0.5))
    model.add(layers.Conv2D(filters=48*filter_size, kernel_size=(5, 5), activation=actfn))
    model.add(MaxPool2D(strides=2))
    model.add(keras.layers.Dropout(0.5))
    model.add(layers.Flatten())
    model.add(layers.Dense(units=256*dense_units_size, activation=actfn))
    model.add(layers.Dense(units=84*dense_units_size, activation=actfn))
    model.add(layers.Dense(units=10, activation = 'softmax'))
    model.compile(loss="categorical_crossentropy", optimizer=optimizer(lr=learningrate), metrics=["accuracy"])   
    return model

leNet_model_size = model_leNet_size_factory()

early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True) 
lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
n_epochs = 100
batch_size = 128
tic = time.time()
LeNet_size_history = leNet_model_size.fit(train_data, train_label, epochs=n_epochs, validation_data=(val_data, val_label),batch_size=batch_size, callbacks=[early_stopping_cb, lr_scheduler])
tok = time.time()
print((tok - tic)*1000 , "ms")

leNet_size_train_acc = np.max(LeNet_size_history.history['accuracy'])
leNet_size_val_acc = np.max(LeNet_size_history.history['val_accuracy'])
print("training datasets accuracy is " , f'{leNet_size_train_acc * 100:.2f}%')
print("validation datasets accuracy is " , f'{leNet_size_val_acc * 100:.2f}%')
plot_history(LeNet_size_history)



# 3, add more layers
# 4, BatchNormalization before every Convolutional Layer.

def model_leNet_layer_factory():
    # LeNet-5
    learningrate=0.001
    actfn = "relu"
    optimizer = keras.optimizers.Adam
    filter_size = 3
    dense_units_size = 2

    model = keras.Sequential()
    model.add(BatchNormalization())
    model.add(layers.Conv2D(filters=32*filter_size, kernel_size=(5, 5), activation=actfn, input_shape=(32,32,3)))
    model.add(BatchNormalization())
    model.add(layers.Conv2D(filters=32*filter_size, kernel_size=(5, 5), activation=actfn, input_shape=(32,32,3)))
    model.add(MaxPool2D(strides=2))
    model.add(keras.layers.Dropout(0.5))
    model.add(BatchNormalization())
    model.add(layers.Conv2D(filters=48*filter_size, kernel_size=(5, 5), activation=actfn))
    model.add(BatchNormalization())
    model.add(layers.Conv2D(filters=48*filter_size, kernel_size=(5, 5), activation=actfn))
    model.add(MaxPool2D(strides=2))
    model.add(keras.layers.Dropout(0.5))
    model.add(layers.Flatten())
    model.add(layers.Dense(units=256*dense_units_size, activation=actfn))
    model.add(layers.Dense(units=84*dense_units_size, activation=actfn))
    model.add(layers.Dense(units=10, activation = 'softmax'))
    model.compile(loss="categorical_crossentropy", optimizer=optimizer(lr=learningrate), metrics=["accuracy"])   
    return model


leNet_model_layer = model_leNet_layer_factory()

early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True) 
lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
n_epochs = 100
batch_size = 128
tic = time.time()
LeNet_layer_history = leNet_model_layer.fit(train_data, train_label, epochs=n_epochs, validation_data=(val_data, val_label),batch_size=batch_size, callbacks=[early_stopping_cb, lr_scheduler])
tok = time.time()
print((tok - tic)*1000 , "ms")

leNet_layer_train_acc = np.max(LeNet_layer_history.history['accuracy'])
leNet_layer_val_acc = np.max(LeNet_layer_history.history['val_accuracy'])
print("training datasets accuracy is " , f'{leNet_layer_train_acc * 100:.2f}%')
print("validation datasets accuracy is " , f'{leNet_layer_val_acc * 100:.2f}%')
plot_history(LeNet_layer_history)

# modified LeNet-5 perform good

leNet_layer_evaluation = leNet_model_layer.evaluate(test_data, test_label)
leNet_layer_test_acc = leNet_layer_evaluation[1]
print("test datasets accuracy is " , f'{leNet_layer_test_acc * 100:.2f}%')

"""### modified VGG
#### we have decrease the dense units and delete a set of convolutional layers and a maximum pool layer due to the small image size
"""

def model_VGG_factory():

    model = Sequential()

    # Convolutional Layer
    model.add(BatchNormalization())
    model.add(Conv2D(filters=64, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))
    model.add(MaxPool2D(pool_size=(2, 2)))
    model.add(keras.layers.Dropout(0.25))

    model.add(BatchNormalization())
    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))
    model.add(MaxPool2D(pool_size=(2, 2)))
    model.add(keras.layers.Dropout(0.25))

    """
    model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))
    model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))
    model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))
    model.add(MaxPool2D(pool_size=(2, 2)))
    """

    model.add(Flatten())
    model.add(Dense(512, activation='relu'))
    model.add(Dense(512, activation='relu'))
    model.add(Dense(10, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

VGG_model = model_VGG_factory()
early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True) 
lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
n_epochs = 100
batch_size = 128
tic = time.time()
VGG_history = VGG_model.fit(train_data, train_label, epochs=n_epochs, validation_data=(val_data, val_label),batch_size=batch_size, callbacks=[early_stopping_cb, lr_scheduler])
tok = time.time()
print((tok - tic)*1000 , "ms")

VGG_train_acc = np.max(VGG_history.history['accuracy'])
VGG_val_acc = np.max(VGG_history.history['val_accuracy'])
print("training datasets accuracy is " , f'{VGG_train_acc * 100:.2f}%')
print("validation datasets accuracy is " , f'{VGG_val_acc * 100:.2f}%')
plot_history(VGG_history)

"""### AlexNet
#### we have decrease the pool size and delete a maximum pool layer due to the small image size
"""

AlexNet_model = keras.models.Sequential([
    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(32,32,3)),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),

    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding="same"),
    keras.layers.BatchNormalization(),
#    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),
    keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),

    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding="same"),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding="same"),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding="same"),
    keras.layers.BatchNormalization(),
#    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),
#    keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),

    keras.layers.Flatten(),
    keras.layers.Dense(4096, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(4096, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(10, activation='softmax')
])
AlexNet_model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])   


early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True) 
lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
n_epochs = 100
batch_size = 128
tic = time.time()
AlexNet_history = AlexNet_model.fit(train_data, train_label, epochs=n_epochs, validation_data=(val_data, val_label),batch_size=batch_size, callbacks=[early_stopping_cb, lr_scheduler])
tok = time.time()
print((tok - tic)*1000 , "ms")

AlexNet_train_acc = np.max(AlexNet_history.history['accuracy'])
AlexNet_val_acc = np.max(AlexNet_history.history['val_accuracy'])
print("training datasets accuracy is " , f'{AlexNet_train_acc * 100:.2f}%')
print("validation datasets accuracy is " , f'{AlexNet_val_acc * 100:.2f}%')
plot_history(AlexNet_history)

"""### ResNet 50"""

# ResNet50
base_model = ResNet50(include_top=False,input_shape=(32,32,3))

x = base_model.output
x = GlobalMaxPooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(100, activation="relu")(x)
x = Dropout(0.5)(x)
x = Dense(10, activation="softmax")(x)
base_model = Model(base_model.input, x, name="base_model")
base_model.compile(loss="binary_crossentropy", metrics=['acc'], optimizer="adam")
base_model.summary()

early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, restore_best_weights=True) 
#early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True) 
lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)

tic = time.time()
history = base_model.fit(train_data, train_label, epochs=100, validation_data=(val_data, val_label),batch_size=512, callbacks=[early_stopping_cb, lr_scheduler])

tok = time.time()
print((tok - tic)*1000 , "ms")

evaluation = base_model.evaluate(test_data, test_label)
train_acc = np.max(history.history['acc'])
val_acc = np.max(history.history['val_acc'])
test_acc = evaluation[1]
print("training datasets accuracy is " , f'{train_acc * 100:.2f}%')
print("validation datasets accuracy is " , f'{val_acc * 100:.2f}%')
print("test datasets accuracy is " , f'{test_acc * 100:.2f}%')
plot_acc_history(history)

"""### Xception
#### Input size must be at least 71x71
"""

# Xception

from keras.applications import Xception
from keras.layers import GlobalMaxPooling2D
from keras import Model

base_model = Xception(include_top=False,input_shape=(32,32,3))

base_model.summary()